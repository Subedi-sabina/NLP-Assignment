{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46d5bb9b-5df4-4349-bdf4-acae2c7a143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aaad4bd-1060-4823-af8e-c73bb492ef3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2364779720.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    nltk.download('averaged_perceptron_tagger'\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e7d89a9-a86a-4750-8a0c-a130b77f258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "['Artificial intelligence is transforming the technology industry.', 'Natural language processing allows computers to understand human language.', 'Machine learning algorithms can detect patterns in large datasets.', 'Deep learning techniques are being used in image recognition and speech synthesis.', 'Data scientists use statistical methods to analyze and interpret complex data.', 'The healthcare industry is adopting AI to improve patient outcomes.', 'Self-driving cars are an application of artificial intelligence in the automotive sector.', 'Climate change is a significant global challenge that requires immediate attention.', 'Renewable energy sources like solar and wind power are essential for sustainable development.', 'The economic impact of the pandemic has been severe, affecting businesses worldwide.', 'Remote work has become more common due to advances in communication technology.', 'Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.', 'Cybersecurity is crucial in protecting sensitive information from digital threats.', 'The entertainment industry is exploring virtual reality to create immersive experiences.', 'Education systems are integrating online learning platforms to enhance accessibility.', 'Space exploration is expanding our understanding of the universe.', 'Genetic research is leading to breakthroughs in personalized medicine.', 'The fashion industry is adopting sustainable practices to reduce environmental impact.', 'Public transportation systems are being upgraded to improve efficiency and reduce emissions.', 'The art world is embracing digital media to reach wider audiences.']\n",
      "\n",
      "Documents:\n",
      "Document 1: Artificial intelligence is transforming the technology industry.\n",
      "Document 2: Natural language processing allows computers to understand human language.\n",
      "Document 3: Machine learning algorithms can detect patterns in large datasets.\n",
      "Document 4: Deep learning techniques are being used in image recognition and speech synthesis.\n",
      "Document 5: Data scientists use statistical methods to analyze and interpret complex data.\n",
      "Document 6: The healthcare industry is adopting AI to improve patient outcomes.\n",
      "Document 7: Self-driving cars are an application of artificial intelligence in the automotive sector.\n",
      "Document 8: Climate change is a significant global challenge that requires immediate attention.\n",
      "Document 9: Renewable energy sources like solar and wind power are essential for sustainable development.\n",
      "Document 10: The economic impact of the pandemic has been severe, affecting businesses worldwide.\n",
      "Document 11: Remote work has become more common due to advances in communication technology.\n",
      "Document 12: Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.\n",
      "Document 13: Cybersecurity is crucial in protecting sensitive information from digital threats.\n",
      "Document 14: The entertainment industry is exploring virtual reality to create immersive experiences.\n",
      "Document 15: Education systems are integrating online learning platforms to enhance accessibility.\n",
      "Document 16: Space exploration is expanding our understanding of the universe.\n",
      "Document 17: Genetic research is leading to breakthroughs in personalized medicine.\n",
      "Document 18: The fashion industry is adopting sustainable practices to reduce environmental impact.\n",
      "Document 19: Public transportation systems are being upgraded to improve efficiency and reduce emissions.\n",
      "Document 20: The art world is embracing digital media to reach wider audiences.\n",
      "\n",
      "Tokens:\n",
      "Tokens for Document 1: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'technology', 'industry']\n",
      "Tokens for Document 2: ['natural', 'language', 'processing', 'allows', 'computers', 'to', 'understand', 'human', 'language']\n",
      "Tokens for Document 3: ['machine', 'learning', 'algorithms', 'can', 'detect', 'patterns', 'in', 'large', 'datasets']\n",
      "Tokens for Document 4: ['deep', 'learning', 'techniques', 'are', 'being', 'used', 'in', 'image', 'recognition', 'and', 'speech', 'synthesis']\n",
      "Tokens for Document 5: ['data', 'scientists', 'use', 'statistical', 'methods', 'to', 'analyze', 'and', 'interpret', 'complex', 'data']\n",
      "Tokens for Document 6: ['the', 'healthcare', 'industry', 'is', 'adopting', 'ai', 'to', 'improve', 'patient', 'outcomes']\n",
      "Tokens for Document 7: ['selfdriving', 'cars', 'are', 'an', 'application', 'of', 'artificial', 'intelligence', 'in', 'the', 'automotive', 'sector']\n",
      "Tokens for Document 8: ['climate', 'change', 'is', 'a', 'significant', 'global', 'challenge', 'that', 'requires', 'immediate', 'attention']\n",
      "Tokens for Document 9: ['renewable', 'energy', 'sources', 'like', 'solar', 'and', 'wind', 'power', 'are', 'essential', 'for', 'sustainable', 'development']\n",
      "Tokens for Document 10: ['the', 'economic', 'impact', 'of', 'the', 'pandemic', 'has', 'been', 'severe', 'affecting', 'businesses', 'worldwide']\n",
      "Tokens for Document 11: ['remote', 'work', 'has', 'become', 'more', 'common', 'due', 'to', 'advances', 'in', 'communication', 'technology']\n",
      "Tokens for Document 12: ['cryptocurrencies', 'like', 'bitcoin', 'are', 'gaining', 'popularity', 'as', 'alternative', 'investments']\n",
      "Tokens for Document 13: ['cybersecurity', 'is', 'crucial', 'in', 'protecting', 'sensitive', 'information', 'from', 'digital', 'threats']\n",
      "Tokens for Document 14: ['the', 'entertainment', 'industry', 'is', 'exploring', 'virtual', 'reality', 'to', 'create', 'immersive', 'experiences']\n",
      "Tokens for Document 15: ['education', 'systems', 'are', 'integrating', 'online', 'learning', 'platforms', 'to', 'enhance', 'accessibility']\n",
      "Tokens for Document 16: ['space', 'exploration', 'is', 'expanding', 'our', 'understanding', 'of', 'the', 'universe']\n",
      "Tokens for Document 17: ['genetic', 'research', 'is', 'leading', 'to', 'breakthroughs', 'in', 'personalized', 'medicine']\n",
      "Tokens for Document 18: ['the', 'fashion', 'industry', 'is', 'adopting', 'sustainable', 'practices', 'to', 'reduce', 'environmental', 'impact']\n",
      "Tokens for Document 19: ['public', 'transportation', 'systems', 'are', 'being', 'upgraded', 'to', 'improve', 'efficiency', 'and', 'reduce', 'emissions']\n",
      "Tokens for Document 20: ['the', 'art', 'world', 'is', 'embracing', 'digital', 'media', 'to', 'reach', 'wider', 'audiences']\n",
      "\n",
      "Vocabulary:\n",
      "{'adopting', 'outcomes', 'processing', 'become', 'significant', 'techniques', 'protecting', 'language', 'of', 'systems', 'cryptocurrencies', 'a', 'impact', 'that', 'improve', 'attention', 'research', 'understanding', 'deep', 'environmental', 'in', 'artificial', 'power', 'used', 'work', 'education', 'accessibility', 'energy', 'sources', 'popularity', 'global', 'patient', 'exploring', 'media', 'the', 'an', 'is', 'audiences', 'challenge', 'immersive', 'algorithms', 'businesses', 'digital', 'human', 'wind', 'due', 'reality', 'bitcoin', 'investments', 'platforms', 'patterns', 'can', 'wider', 'selfdriving', 'fashion', 'ai', 'communication', 'online', 'use', 'like', 'more', 'transportation', 'virtual', 'our', 'automotive', 'worldwide', 'speech', 'remote', 'change', 'leading', 'universe', 'been', 'personalized', 'understand', 'application', 'emissions', 'industry', 'natural', 'data', 'synthesis', 'scientists', 'climate', 'economic', 'reduce', 'learning', 'being', 'enhance', 'upgraded', 'allows', 'image', 'immediate', 'are', 'methods', 'sustainable', 'art', 'interpret', 'complex', 'detect', 'computers', 'and', 'affecting', 'practices', 'as', 'requires', 'essential', 'severe', 'space', 'threats', 'recognition', 'world', 'transforming', 'machine', 'information', 'medicine', 'crucial', 'datasets', 'efficiency', 'common', 'solar', 'expanding', 'integrating', 'embracing', 'create', 'pandemic', 'technology', 'sector', 'alternative', 'for', 'genetic', 'cars', 'public', 'development', 'intelligence', 'entertainment', 'advances', 'sensitive', 'analyze', 'to', 'exploration', 'reach', 'statistical', 'from', 'cybersecurity', 'has', 'healthcare', 'experiences', 'breakthroughs', 'gaining', 'large', 'renewable'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"Artificial intelligence is transforming the technology industry.\",\n",
    "    \"Natural language processing allows computers to understand human language.\",\n",
    "    \"Machine learning algorithms can detect patterns in large datasets.\",\n",
    "    \"Deep learning techniques are being used in image recognition and speech synthesis.\",\n",
    "    \"Data scientists use statistical methods to analyze and interpret complex data.\",\n",
    "    \"The healthcare industry is adopting AI to improve patient outcomes.\",\n",
    "    \"Self-driving cars are an application of artificial intelligence in the automotive sector.\",\n",
    "    \"Climate change is a significant global challenge that requires immediate attention.\",\n",
    "    \"Renewable energy sources like solar and wind power are essential for sustainable development.\",\n",
    "    \"The economic impact of the pandemic has been severe, affecting businesses worldwide.\",\n",
    "    \"Remote work has become more common due to advances in communication technology.\",\n",
    "    \"Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.\",\n",
    "    \"Cybersecurity is crucial in protecting sensitive information from digital threats.\",\n",
    "    \"The entertainment industry is exploring virtual reality to create immersive experiences.\",\n",
    "    \"Education systems are integrating online learning platforms to enhance accessibility.\",\n",
    "    \"Space exploration is expanding our understanding of the universe.\",\n",
    "    \"Genetic research is leading to breakthroughs in personalized medicine.\",\n",
    "    \"The fashion industry is adopting sustainable practices to reduce environmental impact.\",\n",
    "    \"Public transportation systems are being upgraded to improve efficiency and reduce emissions.\",\n",
    "    \"The art world is embracing digital media to reach wider audiences.\"\n",
    "]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize each document in the corpus\n",
    "tokenized_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = set(word for tokens in tokenized_corpus for word in tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Corpus:\")\n",
    "print(corpus)\n",
    "print(\"\\nDocuments:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "print(\"\\nTokens:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    print(f\"Tokens for Document {i+1}: {tokens}\")\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68a37c2d-9ae6-4ace-a9a0-32fa93da199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Stop Words:\n",
      "{'only', 'before', 'isn', \"mightn't\", 'other', 'needn', 'was', 'your', 'am', \"doesn't\", 'of', 'own', 'himself', 'a', 'too', \"should've\", 'that', 'won', 'this', 'further', 'don', 's', \"wasn't\", 'aren', 'had', 'each', 'some', 'in', 'theirs', 'did', \"that'll\", 'there', 'through', \"shouldn't\", 'both', 'will', 'an', 'the', 'is', 'about', 't', 'hadn', 'any', 'does', 'them', 'him', 'until', 'once', 'me', 'you', 'most', 'up', 'here', 'my', 'down', 'against', 'should', \"aren't\", \"needn't\", \"she's\", 'doing', 'how', 'do', 'few', 'herself', 'couldn', 'can', 'on', \"it's\", 'nor', 'into', 'because', 'we', 'more', 'he', 'our', 'so', 'shouldn', 'or', \"you're\", 'their', 'm', 'been', 'doesn', 'below', \"won't\", \"you'd\", 'yourselves', 'off', 'but', 'not', 'shan', \"couldn't\", \"hasn't\", \"didn't\", \"you've\", 'what', 'weren', 'didn', 'have', \"isn't\", 'being', 're', 'ma', \"don't\", 'why', 'hasn', 'her', 'are', 'at', 'over', 'ourselves', 'no', 'she', \"mustn't\", 'and', 'where', 'as', 'wouldn', 'themselves', 'which', 'these', 'between', \"you'll\", 'yourself', 'when', 'hers', 'who', 'while', 'just', 'during', 'ain', 'i', \"wouldn't\", 'all', 've', 'his', 'be', 'll', 'now', 'with', 'yours', 'for', 'same', \"haven't\", 'having', 'again', 'its', 'then', 'whom', 'out', 'such', 'above', \"shan't\", 'wasn', 'd', 'than', 'those', 'they', 'to', 'under', 'mustn', 'were', 'from', 'y', 'mightn', \"weren't\", 'myself', 'ours', 'haven', 'has', 'if', 'itself', 'by', \"hadn't\", 'very', 'after', 'it', 'o'}\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print all stop words\n",
    "print(\"English Stop Words:\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b96e19c0-0387-49a5-8f0d-5d0b6444ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['art', 'world', 'embracing', 'digital', 'media', 'reach', 'wider', 'audiences']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b292321-977e-4291-884c-1f5282c319f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "['Artificial intelligence is transforming the technology industry.', 'Natural language processing allows computers to understand human language.', 'Machine learning algorithms can detect patterns in large datasets.', 'Deep learning techniques are being used in image recognition and speech synthesis.', 'Data scientists use statistical methods to analyze and interpret complex data.', 'The healthcare industry is adopting AI to improve patient outcomes.', 'Self-driving cars are an application of artificial intelligence in the automotive sector.', 'Climate change is a significant global challenge that requires immediate attention.', 'Renewable energy sources like solar and wind power are essential for sustainable development.', 'The economic impact of the pandemic has been severe, affecting businesses worldwide.', 'Remote work has become more common due to advances in communication technology.', 'Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.', 'Cybersecurity is crucial in protecting sensitive information from digital threats.', 'The entertainment industry is exploring virtual reality to create immersive experiences.', 'Education systems are integrating online learning platforms to enhance accessibility.', 'Space exploration is expanding our understanding of the universe.', 'Genetic research is leading to breakthroughs in personalized medicine.', 'The fashion industry is adopting sustainable practices to reduce environmental impact.', 'Public transportation systems are being upgraded to improve efficiency and reduce emissions.', 'The art world is embracing digital media to reach wider audiences.']\n",
      "\n",
      "Documents:\n",
      "Document 1: Artificial intelligence is transforming the technology industry.\n",
      "Document 2: Natural language processing allows computers to understand human language.\n",
      "Document 3: Machine learning algorithms can detect patterns in large datasets.\n",
      "Document 4: Deep learning techniques are being used in image recognition and speech synthesis.\n",
      "Document 5: Data scientists use statistical methods to analyze and interpret complex data.\n",
      "Document 6: The healthcare industry is adopting AI to improve patient outcomes.\n",
      "Document 7: Self-driving cars are an application of artificial intelligence in the automotive sector.\n",
      "Document 8: Climate change is a significant global challenge that requires immediate attention.\n",
      "Document 9: Renewable energy sources like solar and wind power are essential for sustainable development.\n",
      "Document 10: The economic impact of the pandemic has been severe, affecting businesses worldwide.\n",
      "Document 11: Remote work has become more common due to advances in communication technology.\n",
      "Document 12: Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.\n",
      "Document 13: Cybersecurity is crucial in protecting sensitive information from digital threats.\n",
      "Document 14: The entertainment industry is exploring virtual reality to create immersive experiences.\n",
      "Document 15: Education systems are integrating online learning platforms to enhance accessibility.\n",
      "Document 16: Space exploration is expanding our understanding of the universe.\n",
      "Document 17: Genetic research is leading to breakthroughs in personalized medicine.\n",
      "Document 18: The fashion industry is adopting sustainable practices to reduce environmental impact.\n",
      "Document 19: Public transportation systems are being upgraded to improve efficiency and reduce emissions.\n",
      "Document 20: The art world is embracing digital media to reach wider audiences.\n",
      "\n",
      "Tokens:\n",
      "Tokens for Document 1: ['artificial', 'intelligence', 'transforming', 'technology', 'industry']\n",
      "Tokens for Document 2: ['natural', 'language', 'processing', 'allows', 'computers', 'understand', 'human', 'language']\n",
      "Tokens for Document 3: ['machine', 'learning', 'algorithms', 'detect', 'patterns', 'large', 'datasets']\n",
      "Tokens for Document 4: ['deep', 'learning', 'techniques', 'used', 'image', 'recognition', 'speech', 'synthesis']\n",
      "Tokens for Document 5: ['data', 'scientists', 'use', 'statistical', 'methods', 'analyze', 'interpret', 'complex', 'data']\n",
      "Tokens for Document 6: ['healthcare', 'industry', 'adopting', 'ai', 'improve', 'patient', 'outcomes']\n",
      "Tokens for Document 7: ['selfdriving', 'cars', 'application', 'artificial', 'intelligence', 'automotive', 'sector']\n",
      "Tokens for Document 8: ['climate', 'change', 'significant', 'global', 'challenge', 'requires', 'immediate', 'attention']\n",
      "Tokens for Document 9: ['renewable', 'energy', 'sources', 'like', 'solar', 'wind', 'power', 'essential', 'sustainable', 'development']\n",
      "Tokens for Document 10: ['economic', 'impact', 'pandemic', 'severe', 'affecting', 'businesses', 'worldwide']\n",
      "Tokens for Document 11: ['remote', 'work', 'become', 'common', 'due', 'advances', 'communication', 'technology']\n",
      "Tokens for Document 12: ['cryptocurrencies', 'like', 'bitcoin', 'gaining', 'popularity', 'alternative', 'investments']\n",
      "Tokens for Document 13: ['cybersecurity', 'crucial', 'protecting', 'sensitive', 'information', 'digital', 'threats']\n",
      "Tokens for Document 14: ['entertainment', 'industry', 'exploring', 'virtual', 'reality', 'create', 'immersive', 'experiences']\n",
      "Tokens for Document 15: ['education', 'systems', 'integrating', 'online', 'learning', 'platforms', 'enhance', 'accessibility']\n",
      "Tokens for Document 16: ['space', 'exploration', 'expanding', 'understanding', 'universe']\n",
      "Tokens for Document 17: ['genetic', 'research', 'leading', 'breakthroughs', 'personalized', 'medicine']\n",
      "Tokens for Document 18: ['fashion', 'industry', 'adopting', 'sustainable', 'practices', 'reduce', 'environmental', 'impact']\n",
      "Tokens for Document 19: ['public', 'transportation', 'systems', 'upgraded', 'improve', 'efficiency', 'reduce', 'emissions']\n",
      "Tokens for Document 20: ['art', 'world', 'embracing', 'digital', 'media', 'reach', 'wider', 'audiences']\n",
      "\n",
      "Vocabulary:\n",
      "{'adopting', 'outcomes', 'processing', 'become', 'significant', 'techniques', 'protecting', 'language', 'systems', 'cryptocurrencies', 'impact', 'improve', 'attention', 'research', 'understanding', 'deep', 'environmental', 'artificial', 'power', 'used', 'work', 'education', 'accessibility', 'energy', 'sources', 'popularity', 'global', 'patient', 'exploring', 'media', 'audiences', 'challenge', 'immersive', 'algorithms', 'businesses', 'digital', 'human', 'wind', 'due', 'reality', 'bitcoin', 'investments', 'platforms', 'patterns', 'wider', 'selfdriving', 'fashion', 'ai', 'communication', 'online', 'use', 'like', 'transportation', 'virtual', 'automotive', 'worldwide', 'speech', 'remote', 'change', 'leading', 'universe', 'personalized', 'understand', 'application', 'emissions', 'industry', 'natural', 'data', 'synthesis', 'scientists', 'climate', 'economic', 'reduce', 'learning', 'enhance', 'upgraded', 'allows', 'image', 'immediate', 'methods', 'sustainable', 'art', 'interpret', 'complex', 'detect', 'computers', 'affecting', 'practices', 'requires', 'essential', 'severe', 'space', 'threats', 'recognition', 'world', 'transforming', 'machine', 'information', 'medicine', 'crucial', 'datasets', 'efficiency', 'common', 'solar', 'expanding', 'integrating', 'embracing', 'create', 'pandemic', 'technology', 'sector', 'alternative', 'genetic', 'cars', 'public', 'development', 'intelligence', 'entertainment', 'advances', 'sensitive', 'analyze', 'exploration', 'reach', 'statistical', 'cybersecurity', 'healthcare', 'experiences', 'breakthroughs', 'gaining', 'large', 'renewable'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"Artificial intelligence is transforming the technology industry.\",\n",
    "    \"Natural language processing allows computers to understand human language.\",\n",
    "    \"Machine learning algorithms can detect patterns in large datasets.\",\n",
    "    \"Deep learning techniques are being used in image recognition and speech synthesis.\",\n",
    "    \"Data scientists use statistical methods to analyze and interpret complex data.\",\n",
    "    \"The healthcare industry is adopting AI to improve patient outcomes.\",\n",
    "    \"Self-driving cars are an application of artificial intelligence in the automotive sector.\",\n",
    "    \"Climate change is a significant global challenge that requires immediate attention.\",\n",
    "    \"Renewable energy sources like solar and wind power are essential for sustainable development.\",\n",
    "    \"The economic impact of the pandemic has been severe, affecting businesses worldwide.\",\n",
    "    \"Remote work has become more common due to advances in communication technology.\",\n",
    "    \"Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.\",\n",
    "    \"Cybersecurity is crucial in protecting sensitive information from digital threats.\",\n",
    "    \"The entertainment industry is exploring virtual reality to create immersive experiences.\",\n",
    "    \"Education systems are integrating online learning platforms to enhance accessibility.\",\n",
    "    \"Space exploration is expanding our understanding of the universe.\",\n",
    "    \"Genetic research is leading to breakthroughs in personalized medicine.\",\n",
    "    \"The fashion industry is adopting sustainable practices to reduce environmental impact.\",\n",
    "    \"Public transportation systems are being upgraded to improve efficiency and reduce emissions.\",\n",
    "    \"The art world is embracing digital media to reach wider audiences.\"\n",
    "]\n",
    " \n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    " \n",
    "# Tokenize each document in the corpus\n",
    "tokenized_corpus = [preprocess_text(doc) for doc in corpus]\n",
    " \n",
    "# Build vocabulary\n",
    "vocabulary = set(word for tokens in tokenized_corpus for word in tokens)\n",
    " \n",
    "# Print the results\n",
    "print(\"Corpus:\")\n",
    "print(corpus)\n",
    "print(\"\\nDocuments:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "print(\"\\nTokens:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    print(f\"Tokens for Document {i+1}: {tokens}\")\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97d1c3-73e8-49bf-a971-2c906d501710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Porter Stemmer and WordNet Lemmatizerstemmer = PorterStemmer() lemmatizer = WordNetLemmatizer() stop_words = set(stopwords.words('english')) # Sample text corpuscorpus = [     \"The children are playing in the garden.\",     \"She enjoys running every morning.\",     \"He has been a runner for ten years.\",     \"Dogs are loyal animals.\",     \"She fishes every weekend at the lake.\",     \"The fisherman has fished in this river for decades.\",     \"This product is better than the other one.\",     \"She bought a new fishing rod.\"] # Function to get the part of speech tag for lemmatizationdef get_wordnet_pos(word):     tag = nltk.pos_tag([word])[0][1][0].upper()     tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}     return tag_dict.get(tag, wordnet.NOUN) # Function to preprocess text and perform stemming and lemmatizationdef preprocess_text(text):     # Convert to lowercase    text = text.lower()    # Remove punctuation    text = re.sub(r'[^\\w\\s]', '', text)     # Tokenize    tokens = word_tokenize(text)    # Remove stop words    filtered_tokens = [word for word in tokens if word not in stop_words]     # Apply stemming    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]     # Apply lemmatization    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]     return filtered_tokens, stemmed_tokens, lemmatized_tokens # Process each document in the corpusfor document in corpus: filtered_tokens, stemmed_tokens, lemmatized_tokens = preprocess_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1fddedf8-8136-42ce-9216-31cf8a9f5dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: Artificial intelligence is transforming the technology industry.\n",
      "Filtered Tokens: ['artificial', 'intelligence', 'transforming', 'technology', 'industry']\n",
      "Stemmed Tokens: ['artifici', 'intellig', 'transform', 'technolog', 'industri']\n",
      "Lemmatized Tokens: ['artificial', 'intelligence', 'transform', 'technology', 'industry']\n",
      "--------------------------------------------------\n",
      "Original Document: Natural language processing allows computers to understand human language.\n",
      "Filtered Tokens: ['natural', 'language', 'processing', 'allows', 'computers', 'understand', 'human', 'language']\n",
      "Stemmed Tokens: ['natur', 'languag', 'process', 'allow', 'comput', 'understand', 'human', 'languag']\n",
      "Lemmatized Tokens: ['natural', 'language', 'processing', 'allows', 'computer', 'understand', 'human', 'language']\n",
      "--------------------------------------------------\n",
      "Original Document: Machine learning algorithms can detect patterns in large datasets.\n",
      "Filtered Tokens: ['machine', 'learning', 'algorithms', 'detect', 'patterns', 'large', 'datasets']\n",
      "Stemmed Tokens: ['machin', 'learn', 'algorithm', 'detect', 'pattern', 'larg', 'dataset']\n",
      "Lemmatized Tokens: ['machine', 'learn', 'algorithm', 'detect', 'pattern', 'large', 'datasets']\n",
      "--------------------------------------------------\n",
      "Original Document: Deep learning techniques are being used in image recognition and speech synthesis.\n",
      "Filtered Tokens: ['deep', 'learning', 'techniques', 'used', 'image', 'recognition', 'speech', 'synthesis']\n",
      "Stemmed Tokens: ['deep', 'learn', 'techniqu', 'use', 'imag', 'recognit', 'speech', 'synthesi']\n",
      "Lemmatized Tokens: ['deep', 'learn', 'technique', 'use', 'image', 'recognition', 'speech', 'synthesis']\n",
      "--------------------------------------------------\n",
      "Original Document: Data scientists use statistical methods to analyze and interpret complex data.\n",
      "Filtered Tokens: ['data', 'scientists', 'use', 'statistical', 'methods', 'analyze', 'interpret', 'complex', 'data']\n",
      "Stemmed Tokens: ['data', 'scientist', 'use', 'statist', 'method', 'analyz', 'interpret', 'complex', 'data']\n",
      "Lemmatized Tokens: ['data', 'scientist', 'use', 'statistical', 'method', 'analyze', 'interpret', 'complex', 'data']\n",
      "--------------------------------------------------\n",
      "Original Document: The healthcare industry is adopting AI to improve patient outcomes.\n",
      "Filtered Tokens: ['healthcare', 'industry', 'adopting', 'ai', 'improve', 'patient', 'outcomes']\n",
      "Stemmed Tokens: ['healthcar', 'industri', 'adopt', 'ai', 'improv', 'patient', 'outcom']\n",
      "Lemmatized Tokens: ['healthcare', 'industry', 'adopt', 'ai', 'improve', 'patient', 'outcome']\n",
      "--------------------------------------------------\n",
      "Original Document: Self-driving cars are an application of artificial intelligence in the automotive sector.\n",
      "Filtered Tokens: ['selfdriving', 'cars', 'application', 'artificial', 'intelligence', 'automotive', 'sector']\n",
      "Stemmed Tokens: ['selfdriv', 'car', 'applic', 'artifici', 'intellig', 'automot', 'sector']\n",
      "Lemmatized Tokens: ['selfdriving', 'car', 'application', 'artificial', 'intelligence', 'automotive', 'sector']\n",
      "--------------------------------------------------\n",
      "Original Document: Climate change is a significant global challenge that requires immediate attention.\n",
      "Filtered Tokens: ['climate', 'change', 'significant', 'global', 'challenge', 'requires', 'immediate', 'attention']\n",
      "Stemmed Tokens: ['climat', 'chang', 'signific', 'global', 'challeng', 'requir', 'immedi', 'attent']\n",
      "Lemmatized Tokens: ['climate', 'change', 'significant', 'global', 'challenge', 'require', 'immediate', 'attention']\n",
      "--------------------------------------------------\n",
      "Original Document: Renewable energy sources like solar and wind power are essential for sustainable development.\n",
      "Filtered Tokens: ['renewable', 'energy', 'sources', 'like', 'solar', 'wind', 'power', 'essential', 'sustainable', 'development']\n",
      "Stemmed Tokens: ['renew', 'energi', 'sourc', 'like', 'solar', 'wind', 'power', 'essenti', 'sustain', 'develop']\n",
      "Lemmatized Tokens: ['renewable', 'energy', 'source', 'like', 'solar', 'wind', 'power', 'essential', 'sustainable', 'development']\n",
      "--------------------------------------------------\n",
      "Original Document: The economic impact of the pandemic has been severe, affecting businesses worldwide.\n",
      "Filtered Tokens: ['economic', 'impact', 'pandemic', 'severe', 'affecting', 'businesses', 'worldwide']\n",
      "Stemmed Tokens: ['econom', 'impact', 'pandem', 'sever', 'affect', 'busi', 'worldwid']\n",
      "Lemmatized Tokens: ['economic', 'impact', 'pandemic', 'severe', 'affect', 'business', 'worldwide']\n",
      "--------------------------------------------------\n",
      "Original Document: Remote work has become more common due to advances in communication technology.\n",
      "Filtered Tokens: ['remote', 'work', 'become', 'common', 'due', 'advances', 'communication', 'technology']\n",
      "Stemmed Tokens: ['remot', 'work', 'becom', 'common', 'due', 'advanc', 'commun', 'technolog']\n",
      "Lemmatized Tokens: ['remote', 'work', 'become', 'common', 'due', 'advance', 'communication', 'technology']\n",
      "--------------------------------------------------\n",
      "Original Document: Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.\n",
      "Filtered Tokens: ['cryptocurrencies', 'like', 'bitcoin', 'gaining', 'popularity', 'alternative', 'investments']\n",
      "Stemmed Tokens: ['cryptocurr', 'like', 'bitcoin', 'gain', 'popular', 'altern', 'invest']\n",
      "Lemmatized Tokens: ['cryptocurrencies', 'like', 'bitcoin', 'gain', 'popularity', 'alternative', 'investment']\n",
      "--------------------------------------------------\n",
      "Original Document: Cybersecurity is crucial in protecting sensitive information from digital threats.\n",
      "Filtered Tokens: ['cybersecurity', 'crucial', 'protecting', 'sensitive', 'information', 'digital', 'threats']\n",
      "Stemmed Tokens: ['cybersecur', 'crucial', 'protect', 'sensit', 'inform', 'digit', 'threat']\n",
      "Lemmatized Tokens: ['cybersecurity', 'crucial', 'protect', 'sensitive', 'information', 'digital', 'threat']\n",
      "--------------------------------------------------\n",
      "Original Document: The entertainment industry is exploring virtual reality to create immersive experiences.\n",
      "Filtered Tokens: ['entertainment', 'industry', 'exploring', 'virtual', 'reality', 'create', 'immersive', 'experiences']\n",
      "Stemmed Tokens: ['entertain', 'industri', 'explor', 'virtual', 'realiti', 'creat', 'immers', 'experi']\n",
      "Lemmatized Tokens: ['entertainment', 'industry', 'explore', 'virtual', 'reality', 'create', 'immersive', 'experience']\n",
      "--------------------------------------------------\n",
      "Original Document: Education systems are integrating online learning platforms to enhance accessibility.\n",
      "Filtered Tokens: ['education', 'systems', 'integrating', 'online', 'learning', 'platforms', 'enhance', 'accessibility']\n",
      "Stemmed Tokens: ['educ', 'system', 'integr', 'onlin', 'learn', 'platform', 'enhanc', 'access']\n",
      "Lemmatized Tokens: ['education', 'system', 'integrate', 'online', 'learn', 'platform', 'enhance', 'accessibility']\n",
      "--------------------------------------------------\n",
      "Original Document: Space exploration is expanding our understanding of the universe.\n",
      "Filtered Tokens: ['space', 'exploration', 'expanding', 'understanding', 'universe']\n",
      "Stemmed Tokens: ['space', 'explor', 'expand', 'understand', 'univers']\n",
      "Lemmatized Tokens: ['space', 'exploration', 'expand', 'understand', 'universe']\n",
      "--------------------------------------------------\n",
      "Original Document: Genetic research is leading to breakthroughs in personalized medicine.\n",
      "Filtered Tokens: ['genetic', 'research', 'leading', 'breakthroughs', 'personalized', 'medicine']\n",
      "Stemmed Tokens: ['genet', 'research', 'lead', 'breakthrough', 'person', 'medicin']\n",
      "Lemmatized Tokens: ['genetic', 'research', 'lead', 'breakthrough', 'personalize', 'medicine']\n",
      "--------------------------------------------------\n",
      "Original Document: The fashion industry is adopting sustainable practices to reduce environmental impact.\n",
      "Filtered Tokens: ['fashion', 'industry', 'adopting', 'sustainable', 'practices', 'reduce', 'environmental', 'impact']\n",
      "Stemmed Tokens: ['fashion', 'industri', 'adopt', 'sustain', 'practic', 'reduc', 'environment', 'impact']\n",
      "Lemmatized Tokens: ['fashion', 'industry', 'adopt', 'sustainable', 'practice', 'reduce', 'environmental', 'impact']\n",
      "--------------------------------------------------\n",
      "Original Document: Public transportation systems are being upgraded to improve efficiency and reduce emissions.\n",
      "Filtered Tokens: ['public', 'transportation', 'systems', 'upgraded', 'improve', 'efficiency', 'reduce', 'emissions']\n",
      "Stemmed Tokens: ['public', 'transport', 'system', 'upgrad', 'improv', 'effici', 'reduc', 'emiss']\n",
      "Lemmatized Tokens: ['public', 'transportation', 'system', 'upgraded', 'improve', 'efficiency', 'reduce', 'emission']\n",
      "--------------------------------------------------\n",
      "Original Document: The art world is embracing digital media to reach wider audiences.\n",
      "Filtered Tokens: ['art', 'world', 'embracing', 'digital', 'media', 'reach', 'wider', 'audiences']\n",
      "Stemmed Tokens: ['art', 'world', 'embrac', 'digit', 'media', 'reach', 'wider', 'audienc']\n",
      "Lemmatized Tokens: ['art', 'world', 'embrace', 'digital', 'medium', 'reach', 'wider', 'audience']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    " \n",
    "# Initialize the Porter Stemmer and WordNet Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"Artificial intelligence is transforming the technology industry.\",\n",
    "    \"Natural language processing allows computers to understand human language.\",\n",
    "    \"Machine learning algorithms can detect patterns in large datasets.\",\n",
    "    \"Deep learning techniques are being used in image recognition and speech synthesis.\",\n",
    "    \"Data scientists use statistical methods to analyze and interpret complex data.\",\n",
    "    \"The healthcare industry is adopting AI to improve patient outcomes.\",\n",
    "    \"Self-driving cars are an application of artificial intelligence in the automotive sector.\",\n",
    "    \"Climate change is a significant global challenge that requires immediate attention.\",\n",
    "    \"Renewable energy sources like solar and wind power are essential for sustainable development.\",\n",
    "    \"The economic impact of the pandemic has been severe, affecting businesses worldwide.\",\n",
    "    \"Remote work has become more common due to advances in communication technology.\",\n",
    "    \"Cryptocurrencies like Bitcoin are gaining popularity as alternative investments.\",\n",
    "    \"Cybersecurity is crucial in protecting sensitive information from digital threats.\",\n",
    "    \"The entertainment industry is exploring virtual reality to create immersive experiences.\",\n",
    "    \"Education systems are integrating online learning platforms to enhance accessibility.\",\n",
    "    \"Space exploration is expanding our understanding of the universe.\",\n",
    "    \"Genetic research is leading to breakthroughs in personalized medicine.\",\n",
    "    \"The fashion industry is adopting sustainable practices to reduce environmental impact.\",\n",
    "    \"Public transportation systems are being upgraded to improve efficiency and reduce emissions.\",\n",
    "    \"The art world is embracing digital media to reach wider audiences.\"]\n",
    " \n",
    "# Function to get the part of speech tag for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    " \n",
    "# Function to preprocess text and perform stemming and lemmatization\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "    return filtered_tokens, stemmed_tokens, lemmatized_tokens\n",
    " \n",
    "# Process each document in the corpus\n",
    "for document in corpus:\n",
    "    filtered_tokens, stemmed_tokens, lemmatized_tokens = preprocess_text(document)\n",
    "    print(f\"Original Document: {document}\")\n",
    "    print(f\"Filtered Tokens: {filtered_tokens}\")\n",
    "    print(f\"Stemmed Tokens: {stemmed_tokens}\")\n",
    "    print(f\"Lemmatized Tokens: {lemmatized_tokens}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42031a3a-d371-4eec-912b-7eb05228a37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
